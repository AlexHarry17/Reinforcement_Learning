{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cartpole.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexHarry17/Reinforcement_Learning/blob/master/Deep_Learning_Course/Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bZdru119Z2Z",
        "colab_type": "text"
      },
      "source": [
        "Sourced code to learn reinforcement learning from https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDvivcvA9aj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKKy0EdC9ddk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env = env.unwrapped\n",
        "env.seed(1) # Seed is to help with reproducability"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7AiuYYQ9nku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyper params\n",
        "state_size = 4\n",
        "action_size=env.action_space.n\n",
        "\n",
        "max_episodes=1000\n",
        "learning_rate=0.01\n",
        "gamma=0.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_OGoSzX96fE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_normalize_rewards(episode_rewards):\n",
        "  discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "  cumulative = 0.0\n",
        "  for i in reversed(range(len(episode_rewards))):\n",
        "    cumulative = cumulative*gamma*episode_rewards[i]\n",
        "    discounted_episode_rewards[i]=cumulative\n",
        "  mean=np.mean(discounted_episode_rewards)\n",
        "  std=np.std(discounted_episode_rewards)\n",
        "  discounted_episode_rewards=(discounted_episode_rewards-mean)/(std)\n",
        "  return discounted_episode_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzk8T1hq-nwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope('inputs'):\n",
        "  input_=tf.placeholder(tf.float32, [None, state_size], name='input_')\n",
        "  actions=tf.placeholder(tf.int32, [None, action_size], name='actions')\n",
        "  discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name='discounted_episode_rewards')\n",
        "\n",
        "  mean_reward_=tf.placeholder(tf.float32, name='mean_reward')\n",
        "\n",
        "# Create tensorflow layers\n",
        "\n",
        "  with tf.name_scope(\"fc1\"):\n",
        "    fc1 = tf.contrib.layers.fully_connected(inputs=input_, num_outputs=action_size, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "  \n",
        "  with tf.name_scope(\"fc2\"):\n",
        "    fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=action_size, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "  with tf.name_scope(\"fc3\"):\n",
        "    fc3 = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=action_size, activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "  \n",
        "  with tf.name_scope(\"softmax\"):\n",
        "    action_distribution=tf.nn.softmax(fc3)\n",
        "  \n",
        "  with tf.name_scope(\"loss\"):\n",
        "    neg_log_prob=tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc3, labels=actions)\n",
        "    loss = tf.reduce_mean(neg_log_prob*discounted_episode_rewards)\n",
        "\n",
        "  with tf.name_scope(\"train\"):\n",
        "    train_opt=tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DItEkrLKrA1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoard\n",
        "\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
        "\n",
        "tf.summary.scalar(\"Loss\", loss)\n",
        "\n",
        "write_op=tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y6uHhZRrjTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allRewards= []\n",
        "total_rewards = 0\n",
        "maximumRewardRecorded=0\n",
        "episode=0\n",
        "episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for episode in range(max_episodes):\n",
        "    episode_rewards_sum = 0\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # env.render()\n",
        "\n",
        "    while True:\n",
        "      action_propbability_distribution = sess.run(action_distribution, feed_dict={input_:state.reshape([1,4])})\n",
        "\n",
        "      action = np.random.choice(range(action_propbability_distribution.shape[1]), p=action_propbability_distribution.ravel())\n",
        "\n",
        "      # Perform action\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      episode_states.append(state)\n",
        "\n",
        "      action_ = np.zeros(action_size)\n",
        "      action_[action] = 1\n",
        "\n",
        "      episode_actions.append(reward)\n",
        "      episode_rewards.append(reward)\n",
        "\n",
        "      if done:\n",
        "        #calc sum reward\n",
        "\n",
        "        episode_rewards_sum = np.sum(episode_rewards)\n",
        "\n",
        "        allRewards.append(episode_rewards_sum)\n",
        "\n",
        "        total_rewards = np.sum(allRewards)\n",
        "\n",
        "        mean_reward = np.divide(total_rewards, episode+1)\n",
        "\n",
        "        maximumRewardRecorded = np.amax(allRewards)\n",
        "\n",
        "\n",
        "        print(\"==========================================\")\n",
        "        print(\"Episode: \", episode)\n",
        "        print(\"Reward: \", episode_rewards_sum)\n",
        "        print(\"Mean Reward\", mean_reward)\n",
        "        print(\"Max reward so far: \", maximumRewardRecorded)\n",
        "\n",
        "        discounted_episode_rewards = discount_normalize_rewards(episode_rewards)\n",
        "\n",
        "        #         # Feedforward, gradient and backpropagation\n",
        "        # loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "        #                                                   actions: np.vstack(np.array(episode_actions)),\n",
        "        #                                                   discounted_episode_rewards_: discounted_episode_rewards \n",
        "        #                                                 })\n",
        "        \n",
        "        #         # Write TF Summaries\n",
        "        # summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "        #                                                   actions: np.vstack(np.array(episode_actions)),\n",
        "        #                                                   discounted_episode_rewards_: discounted_episode_rewards,\n",
        "        #                                                     mean_reward_: mean_reward\n",
        "        #                                                 })\n",
        "        \n",
        "        # writer.add_summary(summary, episode)\n",
        "        # writer.flush()\n",
        "\n",
        "        episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "        break\n",
        "\n",
        "      if episode % 100 == 0:\n",
        "        saver.save(sess, \"./models/model.ckpt\")\n",
        "        # print(\"Model saved\")\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B19fY_D12WOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Play cartpole\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    env.reset()\n",
        "    rewards = []\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "    for episode in range(10):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards = 0\n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode)\n",
        "\n",
        "        while True:\n",
        "            \n",
        "\n",
        "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
        "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
        "            #print(action_probability_distribution)\n",
        "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "\n",
        "\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            total_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                rewards.append(total_rewards)\n",
        "                print (\"Score\", total_rewards)\n",
        "                break\n",
        "            state = new_state\n",
        "    env.close()\n",
        "    print (\"Score over time: \" +  str(sum(rewards)/10))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}